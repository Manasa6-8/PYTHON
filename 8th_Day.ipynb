{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ddee83e-4e17-4208-9dda-d0107986ed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\manasa\\anaconda\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\manasa\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\manasa\\anaconda\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manasa\\anaconda\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manasa\\anaconda\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\manasa\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bf79eb2-db15-45ff-b4fa-801c0167b5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a example sent.the sentence']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "tok=PunktSentenceTokenizer()\n",
    "input=\"This is a example sent.the sentence\"\n",
    "sent=tok.tokenize(input)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b488890-d3d8-4f61-95ee-56dc7f722c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text=\"hello, world! 123\"\n",
    "text=re.sub(r'[^a-z\\s]','',text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df7fb0da-bb69-4925-9da1-a48ef8dd13b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "unterminated character set at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     filt\u001b[38;5;241m=\u001b[39m[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop]\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filt)\n\u001b[1;32m---> 29\u001b[0m prepro_msg\u001b[38;5;241m=\u001b[39m[preprocess(msg) \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m msag]    \n\u001b[0;32m     30\u001b[0m vec\u001b[38;5;241m=\u001b[39mCountVectorizer()\n\u001b[0;32m     31\u001b[0m x\u001b[38;5;241m=\u001b[39mvec\u001b[38;5;241m.\u001b[39mfit_transform(prepro_msg)\n",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(text):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#Lower case\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m---> 20\u001b[0m     text\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,text)\n\u001b[0;32m     21\u001b[0m     words\u001b[38;5;241m=\u001b[39mtext\u001b[38;5;241m.\u001b[39msplit() \n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\re\\__init__.py:186\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\re\\__init__.py:307\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is an undocumented flag \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout an obvious purpose. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    306\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m--> 307\u001b[0m p \u001b[38;5;241m=\u001b[39m _compiler\u001b[38;5;241m.\u001b[39mcompile(pattern, flags)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m DEBUG:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\re\\_compiler.py:745\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[0;32m    744\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 745\u001b[0m     p \u001b[38;5;241m=\u001b[39m _parser\u001b[38;5;241m.\u001b[39mparse(p, flags)\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    747\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\re\\_parser.py:979\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    976\u001b[0m state\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m=\u001b[39m flags\n\u001b[0;32m    977\u001b[0m state\u001b[38;5;241m.\u001b[39mstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m--> 979\u001b[0m p \u001b[38;5;241m=\u001b[39m _parse_sub(source, state, flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_VERBOSE, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    980\u001b[0m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m=\u001b[39m fix_flags(\u001b[38;5;28mstr\u001b[39m, p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mflags)\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\re\\_parser.py:460\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    458\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 460\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    461\u001b[0m                        \u001b[38;5;129;01mnot\u001b[39;00m nested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items))\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\re\\_parser.py:568\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    566\u001b[0m this \u001b[38;5;241m=\u001b[39m sourceget()\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m this \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munterminated character set\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m here)\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m this \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mset\u001b[39m:\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: unterminated character set at position 0"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "data=[\n",
    "    ['spam','Had your mobile 11 months or more? you are entitled to update to the '],\n",
    "    ['ham','i am gonna be home soon and I dont want to talk about this stuff any more ' ],\n",
    "    ['spam','Congratulations! You have won a $1000 walmart gift card.Call now '],\n",
    "    ['ham','Dont froget to bring the documents today'],\n",
    "]\n",
    "\n",
    "labels=[row[0] for row in data]\n",
    "msag=[row[1] for row in data]\n",
    "\n",
    "def preprocess(text):\n",
    "    #Lower case\n",
    "    text=text.lower()\n",
    "    text=re.sub(r'[^a-z\\w',\"\",text)\n",
    "    words=text.split() \n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stop=set(stopwords.words('english'))\n",
    "    filt=[word for word in words if word not in stop]\n",
    "    return \" \".join(filt)\n",
    "prepro_msg=[preprocess(msg) for msg in msag]    \n",
    "vec=CountVectorizer()\n",
    "x=vec.fit_transform(prepro_msg)\n",
    "y=labels\n",
    "\n",
    "x_train,x_test,y_train,y_text=train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)\n",
    "model=LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred=model.predict(x_test)\n",
    "print(\"Accuracy: \",accuracy_score(y_test,ypred))\n",
    "print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n",
    "new_sms=\"Congratulations! You have won a $1000 walmart gift card.Call now\"\n",
    "new_sms_pre=preprocess(new_sms)\n",
    "new_vec=vec.transform([new_sms_pre])\n",
    "pred=model.predict(new_vec)\n",
    "print(f\"Prediction for new SMS: {pred[0]}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4ceb151-79a6-4a69-bb31-a18dcc9b0fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      1.00      1.00         1\n",
      "        spam       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "Prediction for new SMS: spam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manasa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manasa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manasa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manasa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manasa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "data = [\n",
    "    ['spam', 'Had your mobile 11 months or more? you are entitled to update to the'],\n",
    "    ['ham', 'i am gonna be home soon and I dont want to talk about this stuff any more'],\n",
    "    ['spam', 'Congratulations! You have won a $1000 walmart gift card. Call now'],\n",
    "    ['ham', 'Dont forget to bring the documents today'],\n",
    "]\n",
    "\n",
    "labels = [row[0] for row in data]\n",
    "msgs = [row[1] for row in data]\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', \"\", text) # fixed regex\n",
    "    words = text.split()\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stop = set(stopwords.words('english'))\n",
    "    filt = [word for word in words if word not in stop]\n",
    "    return \" \".join(filt)\n",
    "\n",
    "prepro_msg = [preprocess(msg) for msg in msgs]\n",
    "vec = CountVectorizer()\n",
    "x = vec.fit_transform(prepro_msg)\n",
    "y = labels\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=42, stratify=y)\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "new_sms = \"Congratulations! You have won a $1000 walmart gift card. Call now\"\n",
    "new_sms_pre = preprocess(new_sms)\n",
    "new_vec = vec.transform([new_sms_pre])\n",
    "pred = model.predict(new_vec)\n",
    "print(f\"Prediction for new SMS: {pred[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046995cf-8558-41d0-afe6-ff63a7920150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
